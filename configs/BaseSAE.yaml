model_params:
  name: 'BaseSAE'
  latent_size: 12
  enc_depth: 12 #number of convolutional layers in the encoder
  dec_depth: 12
  pool_every: 2
  unit_dim: 4
  upsampling_factor: 2
  latent_vecs: 128 # for hybrid sampling
  residual_fc: True

opt_params:
  LR: 0.0002
  weight_decay: 0.0
  scheduler_gamma: 0.95
  auto_epochs: 50

data_params:
  dataset_name: MNIST
  batch_size: 144 # Better to have a square number
  test_batch_size: 144
  heldout: False
  real: False
  subset: False #limit number of samples to 100'000 (to avoid memory errors)

trainer_params:
  gpus: 1
  max_epochs: 150

tuner_params:
  num_samples: 1
  cpus: 1

logging_params:
  save_dir: "logs/"
  name: "BaseSAE"
  manual_seed: 1265
  version: standardAE

vis_params:
  save_dir: "vis/"
  plot_every: 10
  num_animations: 5
  num_frames: 5
  fpd: 10
  num_points_irs: 10000


